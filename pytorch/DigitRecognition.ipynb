{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data, Number of images in the dataset:  50559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10000, 784), (10000, 10), (1000, 784), (1000, 10))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def read_csv_into_2d_list(file_name):\n",
    "    data_2d_list = []\n",
    "    with open(file_name, newline='') as f:\n",
    "        csvreader = csv.reader(f)\n",
    "        for row in csvreader:\n",
    "            data_2d_list.append(row)\n",
    "    return data_2d_list\n",
    "\n",
    "mnist_data = read_csv_into_2d_list(\"mnist_train.csv\")\n",
    "print(\"Read data, Number of images in the dataset: \", len(mnist_data))\n",
    "\n",
    "trainAmount = min(len(mnist_data), 10000)\n",
    "\n",
    "X = np.zeros((trainAmount, 784), dtype=np.float32)\n",
    "Y = np.zeros((trainAmount, 10), dtype=np.float32)\n",
    "\n",
    "X_test = np.zeros((1000, 784), dtype=np.float32)\n",
    "Y_test = np.zeros((1000, 10), dtype=np.float32)\n",
    "\n",
    "for i in range(0, trainAmount + 1000):\n",
    "    if i < trainAmount:\n",
    "        for j in range(0, 784):\n",
    "            X[i][j] = float(mnist_data[i][j+1]) / 255.0\n",
    "        Y[i][int(float(mnist_data[i][0]))] = 1.0\n",
    "    else:\n",
    "        for j in range(0, 784):\n",
    "            X_test[i - trainAmount][j] = float(mnist_data[i][j+1]) / 255.0\n",
    "        Y_test[i - trainAmount][int(float(mnist_data[i][0]))] = 1.0\n",
    "\n",
    "\n",
    "X.shape, Y.shape, X_test.shape, Y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data, Number of images in the dataset:  50558\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd #Alternative way to read data using pandas\n",
    "df = pd.read_csv(\"mnist_train.csv\")\n",
    "mnist_data = df.values.tolist()\n",
    "print(\"Read data, Number of images in the dataset: \", len(mnist_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 784), (10000, 10))"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "W1 = np.random.rand(trainAmount, 784) - 0.5\n",
    "b1 = np.random.rand(trainAmount, 1) - 0.5\n",
    "W2 = np.random.rand(10, trainAmount) - 0.5\n",
    "b2 = np.random.rand(trainAmount, 1) - 0.5\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A\n",
    "\n",
    "def ReLU_deriv(Z):\n",
    "    return Z > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward\n",
    "Z1 = np.dot(X, W1.T) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(A1, W2.T) + b2\n",
    "A2 = softmax(Z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backward\n",
    "dZ2 = A2 - Y\n",
    "dW2 = 1 / m * dZ2.T.dot(A1.T)\n",
    "db2 = 1 / m * np.sum(dZ2)\n",
    "dZ1 = W2.T.dot(dZ2.T) * ReLU_deriv(Z1)\n",
    "dW1 = 1 / m * dZ1.dot(X)\n",
    "db1 = 1 / m * np.sum(dZ1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iterations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[392], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m W1, b1, W2, b2 \u001b[39m=\u001b[39m init_params()\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(iterations):\n\u001b[1;32m      3\u001b[0m     Z1, A1, Z2, A2 \u001b[39m=\u001b[39m forward_prop(W1, b1, W2, b2, X)\n\u001b[1;32m      4\u001b[0m     dW1, db1, dW2, db2 \u001b[39m=\u001b[39m backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'iterations' is not defined"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = init_params()\n",
    "for i in range(iterations):\n",
    "    Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "    dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "    W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "    if i % 10 == 0:\n",
    "        print(\"Iteration: \", i)\n",
    "        predictions = get_predictions(A2)\n",
    "        print(get_accuracy(predictions, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10000,784) and (10000,784) not aligned: 784 (dim 1) != 10000 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[391], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m W1, b1, W2, b2 \u001b[39m=\u001b[39m gradient_descent(X, Y, \u001b[39m0.10\u001b[39;49m, \u001b[39m500\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[390], line 11\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(X, Y, alpha, iterations)\u001b[0m\n\u001b[1;32m      9\u001b[0m W1, b1, W2, b2 \u001b[39m=\u001b[39m init_params()\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(iterations):\n\u001b[0;32m---> 11\u001b[0m     Z1, A1, Z2, A2 \u001b[39m=\u001b[39m forward_prop(W1, b1, W2, b2, X)\n\u001b[1;32m     12\u001b[0m     dW1, db1, dW2, db2 \u001b[39m=\u001b[39m backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n\u001b[1;32m     13\u001b[0m     W1, b1, W2, b2 \u001b[39m=\u001b[39m update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
      "Cell \u001b[0;32mIn[309], line 20\u001b[0m, in \u001b[0;36mforward_prop\u001b[0;34m(W1, b1, W2, b2, X)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_prop\u001b[39m(W1, b1, W2, b2, X):\n\u001b[0;32m---> 20\u001b[0m     Z1 \u001b[39m=\u001b[39m W1\u001b[39m.\u001b[39;49mdot(X) \u001b[39m+\u001b[39m b1\n\u001b[1;32m     21\u001b[0m     A1 \u001b[39m=\u001b[39m ReLU(Z1)\n\u001b[1;32m     22\u001b[0m     Z2 \u001b[39m=\u001b[39m W2\u001b[39m.\u001b[39mdot(A1) \u001b[39m+\u001b[39m b2\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (10000,784) and (10000,784) not aligned: 784 (dim 1) != 10000 (dim 0)"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(X, Y, 0.10, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.089\n"
     ]
    }
   ],
   "source": [
    "# Test on last 1000 images\n",
    "correct = 0\n",
    "for n in range(X.shape[0] - 1000, X.shape[0]):\n",
    "    x = torch.matmul(X[n], W) + b\n",
    "    if torch.argmax(x) == torch.argmax(Y[n]):\n",
    "        correct += 1\n",
    "\n",
    "test_accuracy = correct / 1000\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor(4)\n",
      "Output: tensor(4)\n"
     ]
    }
   ],
   "source": [
    "#plot a random image, and the corresponding label\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "n = random.randint(0, X.shape[0])\n",
    "plt.imshow(X[n].view(28, 28), cmap='gray')\n",
    "plt.show()\n",
    "print(\"Label:\", torch.argmax(Y[n]))\n",
    "\n",
    "#forward and print the output\n",
    "x = torch.matmul(X[n], W) + b\n",
    "print(\"Output:\", torch.argmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Loss:  tensor(2.3037)\n",
      "Iteration:  100\n",
      "Loss:  tensor(2.3016)\n",
      "Iteration:  200\n",
      "Loss:  tensor(2.2970)\n",
      "Iteration:  300\n",
      "Loss:  tensor(2.3008)\n",
      "Iteration:  400\n",
      "Loss:  tensor(2.3029)\n",
      "Iteration:  500\n",
      "Loss:  tensor(2.2987)\n",
      "Iteration:  600\n",
      "Loss:  tensor(2.2975)\n",
      "Iteration:  700\n",
      "Loss:  tensor(2.2872)\n",
      "Iteration:  800\n",
      "Loss:  tensor(2.2373)\n",
      "Iteration:  900\n",
      "Loss:  tensor(2.1938)\n",
      "Iteration:  1000\n",
      "Loss:  tensor(2.2429)\n",
      "Iteration:  1100\n",
      "Loss:  tensor(2.1715)\n",
      "Iteration:  1200\n",
      "Loss:  tensor(2.0426)\n",
      "Iteration:  1300\n",
      "Loss:  tensor(2.1257)\n",
      "Iteration:  1400\n",
      "Loss:  tensor(2.0932)\n",
      "Iteration:  1500\n",
      "Loss:  tensor(1.9631)\n",
      "Iteration:  1600\n",
      "Loss:  tensor(1.9556)\n",
      "Iteration:  1700\n",
      "Loss:  tensor(1.8250)\n",
      "Iteration:  1800\n",
      "Loss:  tensor(1.8675)\n",
      "Iteration:  1900\n",
      "Loss:  tensor(1.8950)\n",
      "Iteration:  2000\n",
      "Loss:  tensor(1.7485)\n",
      "Iteration:  2100\n",
      "Loss:  tensor(1.8389)\n",
      "Iteration:  2200\n",
      "Loss:  tensor(1.9910)\n",
      "Iteration:  2300\n",
      "Loss:  tensor(1.8537)\n",
      "Iteration:  2400\n",
      "Loss:  tensor(1.8349)\n",
      "Iteration:  2500\n",
      "Loss:  tensor(1.8233)\n",
      "Iteration:  2600\n",
      "Loss:  tensor(1.7256)\n",
      "Iteration:  2700\n",
      "Loss:  tensor(1.9618)\n",
      "Iteration:  2800\n",
      "Loss:  tensor(1.7805)\n",
      "Iteration:  2900\n",
      "Loss:  tensor(1.7867)\n",
      "Iteration:  3000\n",
      "Loss:  tensor(1.7151)\n",
      "Iteration:  3100\n",
      "Loss:  tensor(1.7853)\n",
      "Iteration:  3200\n",
      "Loss:  tensor(1.7128)\n",
      "Iteration:  3300\n",
      "Loss:  tensor(1.8354)\n",
      "Iteration:  3400\n",
      "Loss:  tensor(1.7387)\n",
      "Iteration:  3500\n",
      "Loss:  tensor(1.7616)\n",
      "Iteration:  3600\n",
      "Loss:  tensor(1.6700)\n",
      "Iteration:  3700\n",
      "Loss:  tensor(1.7379)\n",
      "Iteration:  3800\n",
      "Loss:  tensor(1.7742)\n",
      "Iteration:  3900\n",
      "Loss:  tensor(1.6930)\n"
     ]
    }
   ],
   "source": [
    "#initialize\n",
    "def relu(x):\n",
    "    return torch.max(torch.zeros(x.size()), x)\n",
    "\n",
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.sum(torch.exp(x))\n",
    "\n",
    "Weights1 = torch.randn(784, 10) * 0.0001\n",
    "Bias1 = torch.randn(1, 10) * 0.01\n",
    "Weights2 = torch.randn(10, 10) * 0.0001\n",
    "Bias2 = torch.randn(1, 10) * 0.01\n",
    "X_train = torch.from_numpy(X)\n",
    "Y_train = torch.from_numpy(Y)\n",
    "\n",
    "mini_batch_size = 10\n",
    "\n",
    "for i in range(4000):\n",
    "    #get a random mini-batch\n",
    "    random_indices = torch.randperm(X_train.shape[0])\n",
    "    random_indices = random_indices[:mini_batch_size]\n",
    "    X_train_mini_batch = X_train[random_indices]\n",
    "    Y_train_mini_batch = Y_train[random_indices]\n",
    "\n",
    "    #forward\n",
    "    forwardFirstLayer = torch.matmul(X_train_mini_batch, Weights1) + Bias1\n",
    "    forwardFirstLayer = relu(forwardFirstLayer)\n",
    "    forwardSecondLayer = torch.matmul(forwardFirstLayer, Weights2) + Bias2\n",
    "    forwardSecondLayer = relu(forwardSecondLayer)\n",
    "\n",
    "    #calculate the -logloss\n",
    "    log_probs = torch.log_softmax(forwardSecondLayer, dim=1)\n",
    "    logloss = -torch.sum(log_probs * Y_train_mini_batch) / X_train_mini_batch.shape[0]\n",
    "\n",
    "    #backward\n",
    "    def relu_deriv(x):\n",
    "        return torch.where(x > 0, torch.ones_like(x), torch.zeros_like(x))\n",
    "\n",
    "    def softmax_deriv(x):\n",
    "        exp_x = torch.exp(x)\n",
    "        return exp_x / torch.sum(exp_x, dim=1, keepdim=True) * (1 - exp_x / torch.sum(exp_x, dim=1, keepdim=True))\n",
    "\n",
    "    #calculate the gradients\n",
    "    dlogloss = (forwardSecondLayer - Y_train_mini_batch) * softmax_deriv(forwardSecondLayer)\n",
    "    dWeights2 = torch.matmul(forwardFirstLayer.T, dlogloss) / X_train_mini_batch.shape[0]\n",
    "    dBias2 = torch.sum(dlogloss, 0) / X_train_mini_batch.shape[0]\n",
    "    dforwardFirstLayer = torch.matmul(dlogloss, Weights2.T) * relu_deriv(forwardFirstLayer)\n",
    "    dWeights1 = torch.matmul(X_train_mini_batch.T, dforwardFirstLayer) / X_train_mini_batch.shape[0]\n",
    "    dBias1 = torch.sum(dforwardFirstLayer, 0) / X_train_mini_batch.shape[0]\n",
    "\n",
    "    lr = 0.2\n",
    "\n",
    "    #update the weights\n",
    "    Weights1 = Weights1 - lr * dWeights1\n",
    "    Bias1 = Bias1 - lr * dBias1\n",
    "    Weights2 = Weights2 - lr * dWeights2\n",
    "    Bias2 = Bias2 - lr * dBias2\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"Iteration: \", i)\n",
    "        print(\"Loss: \", logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 89.7\n"
     ]
    }
   ],
   "source": [
    "#test on 1000 last train images\n",
    "correct = 0\n",
    "testImages = torch.from_numpy(X)\n",
    "anserImages = torch.from_numpy(Y)\n",
    "for n in range(X.shape[0] - 1000, X.shape[0]):\n",
    "    forwardFirstLayer = torch.matmul(testImages[n], Weights1) + Bias1\n",
    "    forwardFirstLayer = relu(forwardFirstLayer)\n",
    "    forwardSecondLayer = torch.matmul(forwardFirstLayer, Weights2) + Bias2\n",
    "    if torch.argmax(forwardSecondLayer) == torch.argmax(anserImages[n]):\n",
    "        correct += 1\n",
    "\n",
    "test_accuracy = correct / 10\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.6\n"
     ]
    }
   ],
   "source": [
    "#test on 1000 test images\n",
    "correct = 0\n",
    "testImages = torch.from_numpy(X_test)\n",
    "anserImages = torch.from_numpy(Y_test)\n",
    "for n in range(0, 1000):\n",
    "    forwardFirstLayer = torch.matmul(testImages[n], Weights1) + Bias1\n",
    "    forwardFirstLayer = relu(forwardFirstLayer)\n",
    "    forwardSecondLayer = torch.matmul(forwardFirstLayer, Weights2) + Bias2\n",
    "    if torch.argmax(forwardSecondLayer) == torch.argmax(anserImages[n]):\n",
    "        correct += 1\n",
    "\n",
    "test_accuracy = correct / 10\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
